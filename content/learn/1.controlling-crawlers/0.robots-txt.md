---
title: Robots.txt in Vue & Nuxt
description: Learn why, when and how to implement a best practice robots.txt in your Vue and Nuxt apps.
navigation:
  title: 'Robots.txt'
publishedAt: 2024-11-03
updatedAt: 2024-11-03
readTime: 8 mins
keywords:
  - robots.txt
  - vue
  - nuxt
  - crawler control
---

## Introduction

The `robots.txt` file lives in your root web directory is a common way to [control how crawlers access your site](/learn/controlling-crawlers).

**✅ Good for:**

- Blocking large site sections (e.g., /admin/*)
- Managing crawler bandwidth on heavy pages (e.g., search, infinite scroll)
- Preventing crawling of development sites

**❌ Don't use for:**

- Protecting sensitive data (crawlers can ignore rules)
- Individual page indexing (use meta robots instead)
- Removing existing pages from search results

Implementing `robots.txt` is straightforward, you can either create a static file or a dynamic one for Vue / Nuxt applications.

## Quick Setup

To get started quickly with a static `robots.txt`, add the file in your public directory:

```dir
public/
  robots.txt
```

Add your rules:

```robots-txt [robots.txt]
# Allow all crawlers
User-agent: *
Disallow:

# Optionally point to your sitemap
Sitemap: https://mysite.com/sitemap.xml
```

### Dynamic Implementation

In some cases, you may prefer a dynamic `robots.txt` file. That is, we server-side generate the file based on the environment or other factors.

::code-group

```ts [Vue]
// example using Vite SSR
function createServer() {
  const app = express()
  // ..
  app.get('/robots.txt', (req, res) => {
    const robots = `
      User-agent: *
      Disallow: /admin
    `
    res.type('text/plain').send(robots)
  })
  // ..
}
```

```ts [Nuxt]
import { getRequestHost } from 'h3'

// server/routes/robots.txt.ts
export default defineEventHandler((e) => {
  const host = getRequestHost(e)
  return host.includes('staging')
    ? 'User-agent: *\nDisallow: /'
    : 'User-agent: *\nDisallow:'
})
```

::

Using Nuxt? The [Nuxt Robots](/modules/robots) module can handle this automatically.

:ModuleCard{slug="robots" class="w-1/2"}

## Understanding robots.txt

The `robots.txt` file consists of these main directives:

```robots-txt [robots.txt]
# Define which crawler these rules apply to
User-agent: *

# Block access to specific paths
Disallow: /admin

# Allow access to specific paths (optional)
Allow: /admin/public

# Point to your sitemap
Sitemap: https://mysite.com/sitemap.xml
```

### User-agent

The `User-agent` directive specifies which crawler the rules apply to:

```robots-txt [robots.txt]
# All crawlers
User-agent: *

# Just Googlebot
User-agent: Googlebot

# Multiple specific crawlers
User-agent: Googlebot
User-agent: Bingbot
Disallow: /private
```

Common crawler user agents:
- [Googlebot](https://developers.google.com/search/docs/advanced/crawling/overview-google-crawlers): Google's crawler
- [Bingbot](https://ahrefs.com/seo/glossary/bingbot): Microsoft's crawler
- [FacebookExternalHit](https://developers.facebook.com/docs/sharing/webmasters/web-crawlers/): Facebook's crawler
- [GPTBot](https://platform.openai.com/docs/bots/overview-of-openai-crawlers): OpenAI's crawler
- [Claude-Web](https://darkvisitors.com/agents/claude-web): Anthropic's crawler

### Allow / Disallow

The `Allow` and `Disallow` directives control path access:

```robots-txt [robots.txt]
User-agent: *
# Block all paths starting with /admin
Disallow: /admin

# Block a specific file
Disallow: /private.html

# Block files with specific extensions
Disallow: /*.pdf$

# Block URL parameters
Disallow: /*?*
```

Path matching uses simple pattern matching:
- `*` matches any sequence of characters
- `$` matches the end of the URL
- Paths are relative to the root domain

### Sitemap

The `Sitemap` directive tells crawlers where to find your [sitemap.xml](/learn/controlling-crawlers/sitemaps):

```robots-txt [robots.txt]
Sitemap: https://mysite.com/sitemap.xml

# Multiple sitemaps
Sitemap: https://mysite.com/products-sitemap.xml
Sitemap: https://mysite.com/blog-sitemap.xml
```

### Yandex Directives

The Yandex search engine introduced additional directives, of which only `Clean-Param` is useful.

- `Clean-Param`: Removes URL parameters from the URL before crawling
- `Host`: Specifies the host name of the site (unused)
- `Crawl-Delay`: Specifies the delay between requests (unused)

If you need to use this, you should target the Yandex user agent:

```robots-txt [robots.txt]
# Remove URL parameters
User-Agent: Yandex
Clean-Param: param1 param2
```

### Security Considerations

- `robots.txt` is publicly visible - avoid revealing sensitive URL patterns
- Not all crawlers follow the rules - see our [security guide](/learn/controlling-crawlers/security)

### SEO Impact

- Blocking search crawlers prevents indexing but doesn't remove existing pages
- For page-level control, use [meta robots tags](/learn/controlling-crawlers/meta-tags) instead
- Blocked resources can affect page rendering and SEO

### Common Mistakes

1. **Blocking CSS/JS/Assets**

```robots-txt [robots.txt]
# ❌ May break page rendering
User-agent: *
Disallow: /assets
Disallow: /css
```

2. **Using robots.txt for Authentication**

```robots-txt [robots.txt]
# ❌ Not secure
User-agent: *
Disallow: /admin
```

3. **Blocking Site Features**

```robots-txt [robots.txt]
# ❌ Better to use meta robots
User-agent: *
Disallow: /search
```

## Testing

### Using Google's Tools

1. Visit [Google's robots.txt Tester](https://www.google.com/webmasters/tools/robots-testing-tool)
2. Add your site
3. Test specific URLs

## Common Patterns

### Allow Everything (Default)

```robots-txt
User-agent: *
Disallow:
```

### Block Everything

Useful for staging or development environments.

```robots-txt
User-agent: *
Disallow: /
```
See our [security guide](/learn/controlling-crawlers/security) for more on environment protection.

### Block AI Crawlers

```robots-txt
User-agent: GPTBot
User-agent: Claude-Web
User-agent: CCBot
User-agent: Google-Extended
Disallow: /
```

### Block Search While Allowing Social

```robots-txt
# Block search engines
User-agent: Googlebot
User-agent: Bingbot
Disallow: /

# Allow social crawlers
User-agent: facebookexternalhit
User-agent: Twitterbot
Allow: /
```

### Block Heavy Pages

```robots-txt
User-agent: *
# Block search results
Disallow: /search
# Block filter pages
Disallow: /products?*
# Block print pages
Disallow: /*/print
```

## Related

- [Meta Robots Guide](/learn/controlling-crawlers/meta-tags) - Page-level crawler control
- [Sitemaps Guide](/learn/controlling-crawlers/sitemaps) - Telling crawlers about your pages
- [Security Guide](/learn/controlling-crawlers/security) - Protecting from malicious crawlers
